{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"I'm currently a researcher at the National Audiovisual Institute (INA) in Paris metropolitan area (France). At INA, I study machine learning algorithms applied to various digital humanities' tasks, such as natural language processing (e.g., fake news analysis and detection, named entities disambiguation) and speech processing (e.g., speaker diarization, political speech style segmentation). The techniques I employ range from standard machine learning algorithms (e.g., GMM, SVM, K-Means, or Bayesian networks), to deep neural networks (e.g., LSTM, CNN). The main tools I use are TensorFlow , Keras , Scikit-learn , NumPy-SciPy , Pandas , and JupyterLab . Prior to my current position, I worked in signal processing as an electronic and computer science engineer. I then gradually switched to machine learning since 2012, when I started my Ph.D. in speech synthesis at the LIMSI-CNRS from the University of Paris-Sud (now Paris-Saclay ). Written in French and defended in September 2015, it concerned expressive speech synthesis. My research interest range from speech and natural language processing (NLP) to auditory spatial perception : Speech analysis and synthesis, phonetics, prosody Natural language processing, machine learning, deep learning Digital signal processing, audio, acoustics, spatial hearing","title":"Home"},{"location":"3d-audio/","text":"University of Paris-Sud \u2014 Year 2 CS-Master (TC-2) Theoretical and practical material for the Fundamentals of Virtual and Augmented Reality class (TC2, Master 2 Research in Human-Computer Interaction). Download class part 1 Download class part 2 Download practical material Pure Data practical instruction (2012) Here is the binaural mixing program to complete. Try to finish it for this Thursday, before the next practical. Please feel free to contact me through email if you have any questions. I also included a WAV file of an audio example to be processed. It is a voice that I recorded some years ago. The recording is completely raw without any effect. You can use any kind of other sounds that you wish as long as it is a mono wave file without effects. Some information: Stages (1), (2) and (3) are finalized. You just need to add a stage (4), which will handle the filtering. You should first use a function to read the arraySrc, like tabplay~ . To apply the filter you have to use a convolution. You may apply it in the time or the frequency domain. The frequency domain is usually more efficient as it allows the use of the FFT (Fast Fourier Transform). Although FIR~ is chiefly designed to apply a Finite Impulse Response filter, it is also capable of applying a simple FFT convolution efficiently. An important point is that HRTF does not provide the intensity decrease with the distance following the Inverse Square Law . You should then use the dist_m variable from the stage (2) to apply this effect in the stage (4). To use a variable in Pd you can use the function r (which stands for \"receive\") followed by the variable name. The last stage should be a dac~ module to transmit the sound to your audio interface. Don't forget to tick the compute audio option in the main application window to hear some sound. These explanations are rather long, but the stage (4) is rather small and straightforward if you follow these steps. Extra: If you wish to observe the shape of the HRTF (HRIR actually) file, you can use Audacity to import it: File > Import > Raw Data... Use: 32 bit float; Little-endian; mono; 16 bytes offset; 44100 sample rate Good luck to you all!","title":"3D-Audio Class"},{"location":"3d-audio/#university-of-paris-sud-year-2-cs-master-tc-2","text":"Theoretical and practical material for the Fundamentals of Virtual and Augmented Reality class (TC2, Master 2 Research in Human-Computer Interaction). Download class part 1 Download class part 2 Download practical material","title":"University of Paris-Sud \u2014 Year 2 CS-Master (TC-2)"},{"location":"3d-audio/#pure-data-practical-instruction-2012","text":"Here is the binaural mixing program to complete. Try to finish it for this Thursday, before the next practical. Please feel free to contact me through email if you have any questions. I also included a WAV file of an audio example to be processed. It is a voice that I recorded some years ago. The recording is completely raw without any effect. You can use any kind of other sounds that you wish as long as it is a mono wave file without effects.","title":"Pure Data practical instruction (2012)"},{"location":"3d-audio/#some-information","text":"Stages (1), (2) and (3) are finalized. You just need to add a stage (4), which will handle the filtering. You should first use a function to read the arraySrc, like tabplay~ . To apply the filter you have to use a convolution. You may apply it in the time or the frequency domain. The frequency domain is usually more efficient as it allows the use of the FFT (Fast Fourier Transform). Although FIR~ is chiefly designed to apply a Finite Impulse Response filter, it is also capable of applying a simple FFT convolution efficiently. An important point is that HRTF does not provide the intensity decrease with the distance following the Inverse Square Law . You should then use the dist_m variable from the stage (2) to apply this effect in the stage (4). To use a variable in Pd you can use the function r (which stands for \"receive\") followed by the variable name. The last stage should be a dac~ module to transmit the sound to your audio interface. Don't forget to tick the compute audio option in the main application window to hear some sound. These explanations are rather long, but the stage (4) is rather small and straightforward if you follow these steps.","title":"Some information:"},{"location":"3d-audio/#extra","text":"If you wish to observe the shape of the HRTF (HRIR actually) file, you can use Audacity to import it: File > Import > Raw Data... Use: 32 bit float; Little-endian; mono; 16 bytes offset; 44100 sample rate Good luck to you all!","title":"Extra:"},{"location":"audio-dsp/","text":"University of Paris-Sud \u2014 Year 5 CS-Engineers (ET-5) Theoretical and practical material files for the 2014-2015 Polytech\u2019 Paris-Sud Et5 Audio signal processing class. They are under the Ipython Notebook 2 format. Download class 1 material Download class 2 material Download class 3 material Download class 4 material","title":"Audio-DSP Class"},{"location":"audio-dsp/#university-of-paris-sud-year-5-cs-engineers-et-5","text":"Theoretical and practical material files for the 2014-2015 Polytech\u2019 Paris-Sud Et5 Audio signal processing class. They are under the Ipython Notebook 2 format. Download class 1 material Download class 2 material Download class 3 material Download class 4 material","title":"University of Paris-Sud \u2014 Year 5 CS-Engineers (ET-5)"},{"location":"contact/","text":"mevrard@ina.fr","title":"Contact"},{"location":"contact/#amp109amp101amp118amp114amp97amp114amp100amp64amp105amp110amp97amp46amp102amp114","text":"","title":"\u0002amp\u0003#109;\u0002amp\u0003#101;\u0002amp\u0003#118;\u0002amp\u0003#114;\u0002amp\u0003#97;\u0002amp\u0003#114;\u0002amp\u0003#100;\u0002amp\u0003#64;\u0002amp\u0003#105;\u0002amp\u0003#110;\u0002amp\u0003#97;\u0002amp\u0003#46;\u0002amp\u0003#102;\u0002amp\u0003#114;"},{"location":"covid-19-resources/","text":"World Johns Hopkins Coronavirus Resource \u2013 COVID-19 Map Worldometer \u2013 Coronavirus Update Aatish Bhatia \u2013 Trajectory of World COVID-19 Confirmed Cases ECDC \u2013 COVID-19 situation update for the EU/EEA and the UK France French government \u2013 Covid-19 data and maps Le Monde \u2013 Hospitalized number per French department Belgium Sciensano \u2013 Epistat Covid-19 Japan Ministry of Health, Labour and Welfare \u2013 About Coronavirus Disease 2019 (COVID-19) Shane Reustle \u2013 Japan COVID-19 Coronavirus Tracker","title":"Covid-19 Resources"},{"location":"covid-19-resources/#world","text":"Johns Hopkins Coronavirus Resource \u2013 COVID-19 Map Worldometer \u2013 Coronavirus Update Aatish Bhatia \u2013 Trajectory of World COVID-19 Confirmed Cases ECDC \u2013 COVID-19 situation update for the EU/EEA and the UK","title":"World"},{"location":"covid-19-resources/#france","text":"French government \u2013 Covid-19 data and maps Le Monde \u2013 Hospitalized number per French department","title":"France"},{"location":"covid-19-resources/#belgium","text":"Sciensano \u2013 Epistat Covid-19","title":"Belgium"},{"location":"covid-19-resources/#japan","text":"Ministry of Health, Labour and Welfare \u2013 About Coronavirus Disease 2019 (COVID-19) Shane Reustle \u2013 Japan COVID-19 Coronavirus Tracker","title":"Japan"},{"location":"cpp/","text":"University of Paris-Sud \u2014 Year 4 CS-Engineers (ET-4) Practicals and project presentation PDF files for the 2014-2015 Polytech\u2019 Paris-Sud Et4 info class. Download practical 1 Download practical 2 Download practical 3 Download practical 4 Download project Eclipse-EGit Tutorial","title":"C++ Class"},{"location":"cpp/#university-of-paris-sud-year-4-cs-engineers-et-4","text":"Practicals and project presentation PDF files for the 2014-2015 Polytech\u2019 Paris-Sud Et4 info class. Download practical 1 Download practical 2 Download practical 3 Download practical 4 Download project Eclipse-EGit Tutorial","title":"University of Paris-Sud \u2014 Year 4 CS-Engineers (ET-4)"},{"location":"cv/","text":"Doctor of Science in Computer Science Status (since 2018): Research and development engineer at the National Audiovisual Institute (INA) , in France. Domains: Deep learning, Speech processing, NLP, digital humanities. Contact: mevrard@ina.fr Education 2012 -- 2015: Ph.D. in Computer Science at the LIMSI-CNRS -- University of Paris-Sud University of Paris-Sud (now Paris-Saclay ), in France. 2010: Preliminary complement to postgraduate education at the University of Li\u00e8ge (ULi\u00e8ge) , in Belgium. 2007: Audio engineering diploma at the SAE Institute Byron Bay , in Australia. 2002 -- 2004: Complementary Master in Business Administration at the Universit\u00e9 Libre de Bruxelles (ULB) , in Belgium. 1997 -- 2002: Master of Science in Industrial engineering at the Gramme Institute (now in HELMo) , in Belgium. Languages French: First language. English: Full professional proficiency. Dutch, Japanese: Basic. Other activities and Hobbies Music: Guitar, bass, and keyboard player in various rock and jazz bands since 1990. Audio engineering: Soundtrack production of 2 animated short-films (with Camera-etc , Li\u00e8ge, Belgium). Recording, mixing, and production of Rock, Jazz , and Electronic music . Other: Photography, cinema, reading, martial arts.","title":"Short CV"},{"location":"cv/#education","text":"2012 -- 2015: Ph.D. in Computer Science at the LIMSI-CNRS -- University of Paris-Sud University of Paris-Sud (now Paris-Saclay ), in France. 2010: Preliminary complement to postgraduate education at the University of Li\u00e8ge (ULi\u00e8ge) , in Belgium. 2007: Audio engineering diploma at the SAE Institute Byron Bay , in Australia. 2002 -- 2004: Complementary Master in Business Administration at the Universit\u00e9 Libre de Bruxelles (ULB) , in Belgium. 1997 -- 2002: Master of Science in Industrial engineering at the Gramme Institute (now in HELMo) , in Belgium.","title":"Education"},{"location":"cv/#languages","text":"French: First language. English: Full professional proficiency. Dutch, Japanese: Basic.","title":"Languages"},{"location":"cv/#other-activities-and-hobbies","text":"Music: Guitar, bass, and keyboard player in various rock and jazz bands since 1990. Audio engineering: Soundtrack production of 2 animated short-films (with Camera-etc , Li\u00e8ge, Belgium). Recording, mixing, and production of Rock, Jazz , and Electronic music . Other: Photography, cinema, reading, martial arts.","title":"Other activities and Hobbies"},{"location":"publi/","text":"M. Evrard, R. Uro, N. Herv\u00e9, and B. Mazoyer, \u201cFrench Tweet Corpus for Automatic StanceDetection,\u201d International Conference on Language Resources and Evaluation (LREC), 2020. R. Uro, M. Evrard, N. Herv\u00e9, and B. Mazoyer, \u201cThe Constitution of a French Tweet Corpus for Automatic Stance Detection,\u201d SLSP 2019, Ljubljana, Slovenia, 2019. D. Doukhan, E. Lechapt, M. Evrard, and J. Carrive, \u201cINA\u2019S MIREX 2018 music and speech detection system,\u201d in Music Information Retrieval Evaluation eXchange (MIREX), 2018. A. Rilliard, C. d\u2019Alessandro, and M. Evrard, \u201cParadigmatic variation of vowels in expressive speech: acoustic description and dimensional analysis,\u201d in Journal of the Acoustical Society of Americal (Jasa), 2018. M. Evrard, M. Miwa. and Y. Sasaki, \u201cSemantic graph embeddings and a neural language model for WSD,\u201d Second International Workshop on Symbolic-Neural Learning (SNL), 2018. M. Evrard, M. Miwa. and Y. Sasaki, \u201cTTI's Approaches to Symbolic-Neural Learning,\u201d First International Workshop on Symbolic-Neural Learning (SNL), 2017. M. Evrard, \u201cSynth\u00e8se de parole expressive \u00e0 partir du texte: Des phonostyles au contr\u00f4le gestuel pour la synth\u00e8se param\u00e9trique statistique,\u201d PhD thesis, Universit\u00e9 de Paris-Sud, 2015. M. Evrard, S. Delalez, C. d\u2019Alessandro, and A. Rilliard, \u201cComparison of chironomic stylization versus statistical modeling of prosody for expressive speech synthesis,\u201d in Sixteenth Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015. M. Evrard, C. d\u2019Alessandro, and A. Rilliard, \u201cEvaluation of the impact of corpus phonetic alignment on the hmm-based speech synthesis quality,\u201d in Statistical Language and Speech Processing, Springer, 2015. C.-T. Do, M. Evrard, A. Leman, C. d\u2019Alessandro, A. Rilliard, and J.-L. Crebouw, \u201cObjective evaluation of hmm-based speech synthesis system using Kullback-Leibler divergence,\u201d in Fifteenth Annual Conference of the International Speech Communication Association (INTERSPEECH), 2014. M. Evrard, C. R. Andr\u00e9, J. G. Verly, J.-J. Embrechts, and B. F. Katz, \u201cObject-based sound re-mix for the spatially coherent audio rendering of an existing stereoscopic-3D animation movie,\u201d in Audio Engineering Society Convention 131, Audio Engineering Society, New York, NY, United States, 2011. M. Evrard, C. Andr\u00e9, J. Verly, and J.-J. Embrechts, \u201cAdding wave-field-synthesis 3D audio to an existing stereoscopic-3D animation movie,\u201d in Third edition of the 3D Stereo MEDIA international summit, Li\u00e8ge, Belgium, 2011. M. Evrard, C. Andr\u00e9, J.-J. Embrechts, and J. Verly, \u201c3D audio acquisition and reproduction systems,\u201d in Journ\u00e9e ABAV (Association Belge des Acousticiens), Neder-over-Heembeek, Belgium, 2011. M. Evrard, A. Rilliard, and C. d\u2019Alessandro, \u201cReproduction de la personnalit\u00e9 vocale d\u2019un acteur,\u201d in Journ\u00e9es Jeunes Chercheurs en Audition, Acoustique musicale, et Signal audio (JJCAAS), Marseille, France, 2012. M. Evrard, A. Rilliard, and C. d\u2019Alessandro, \u201cCaract\u00e9risation et reproduction de la personnalit\u00e9 vocale d\u2019un acteur,\u201d in Journ\u00e9e des Doctorants du LIMSI-CNRS (JDD), Orsay, France, 2012.","title":"Publications"},{"location":"thesis/","text":"Expressive Text-to-Speech Synthesis From Phonostyles to Gestural Control for Parametric Statistic Synthesis (Written in French, title and abstract are here translated in English) The subject of this thesis was the study and conception of a platform for expressive speech synthesis. The LIPS\u00b3 Text-to-Speech system \u2014 developed in the context of this thesis \u2014 includes a linguistic module and a parametric statistical module (built upon HTS and STRAIGHT). The system was based on a new single-speaker corpus, designed, recorded and annotated. The first study analyzed the influence of the precision of the training corpus phonetic labeling on the synthesis quality. It showed that statistical parametric synthesis is robust to labeling and alignment errors. This addresses the issue of variation in phonetic realizations for expressive speech. The second study presents an acoustic-phonetic analysis of the corpus, characterizing the expressive space used by the speaker to instantiate the instructions that described the different expressive conditions. Voice source parameters and articulatory settings were analyzed according to their phonetic classes, which allowed for a fine phonostylistic characterization. The third study focused on intonation and rhythm. Calliphony 2.0 is a real-time chironomic interface that controls the f0 and rhythmic parameters of prosody, using drawing/writing hand gestures with a stylus and a graphics tablet. These hand-controlled modulations are used to enhance the TTS output, producing speech that is more realistic, without degradation as it is directly applied to the vocoder parameters. Intonation and rhythm stylization using this interface brings significant improvement to the prototypicality of expressivity, as well as to the general quality of synthetic speech. These studies show that parametric statistical synthesis, combined with a chironomic interface, offers an efficient solution for expressive speech synthesis, as well as a powerful tool for the study of prosody. Keywords: Expressive speech synthesis, gestural control, prosody, parametric statistical speech synthesis, adaptative training, HTS.","title":"Thesis"},{"location":"thesis/#expressive-text-to-speech-synthesis","text":"","title":"Expressive Text-to-Speech Synthesis"},{"location":"thesis/#from-phonostyles-to-gestural-control-for-parametric-statistic-synthesis","text":"(Written in French, title and abstract are here translated in English) The subject of this thesis was the study and conception of a platform for expressive speech synthesis. The LIPS\u00b3 Text-to-Speech system \u2014 developed in the context of this thesis \u2014 includes a linguistic module and a parametric statistical module (built upon HTS and STRAIGHT). The system was based on a new single-speaker corpus, designed, recorded and annotated. The first study analyzed the influence of the precision of the training corpus phonetic labeling on the synthesis quality. It showed that statistical parametric synthesis is robust to labeling and alignment errors. This addresses the issue of variation in phonetic realizations for expressive speech. The second study presents an acoustic-phonetic analysis of the corpus, characterizing the expressive space used by the speaker to instantiate the instructions that described the different expressive conditions. Voice source parameters and articulatory settings were analyzed according to their phonetic classes, which allowed for a fine phonostylistic characterization. The third study focused on intonation and rhythm. Calliphony 2.0 is a real-time chironomic interface that controls the f0 and rhythmic parameters of prosody, using drawing/writing hand gestures with a stylus and a graphics tablet. These hand-controlled modulations are used to enhance the TTS output, producing speech that is more realistic, without degradation as it is directly applied to the vocoder parameters. Intonation and rhythm stylization using this interface brings significant improvement to the prototypicality of expressivity, as well as to the general quality of synthetic speech. These studies show that parametric statistical synthesis, combined with a chironomic interface, offers an efficient solution for expressive speech synthesis, as well as a powerful tool for the study of prosody. Keywords: Expressive speech synthesis, gestural control, prosody, parametric statistical speech synthesis, adaptative training, HTS.","title":"From Phonostyles to Gestural Control for Parametric Statistic Synthesis"}]}