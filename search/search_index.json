{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"I'm currently a researcher at the National Audiovisual Institute (INA) in Paris area (France). At INA, I study machine learning algorithms applied to various digital humanities' tasks, such as natural language processing (e.g., fake news analysis and detection, named entities disambiguation) and speech processing (e.g., speaker diarization, political speech style segmentation). The techniques I employ range from standard machine learning algorithms (e.g., GMM, SVM, K-Means, or Bayesian networks), to deep neural networks (e.g., LSTM, CNN). The main tools I use are TensorFlow, Keras, Scikit-learn, NumPy-SciPy, Pandas, and JupyterLab. Prior to my current position, I worked in signal processing as an electronic and computer science engineer. I then gradually switched to machine learning since 2012, when I started my Ph.D. in speech synthesis at the LIMSI-CNRS from the University of Paris-Sud. Written in French and defended in September 2015, it concerned expressive speech synthesis. My research interest range from speech and natural language processing (NLP) to auditory spatial perception : Natural language processing, machine learning Synthesis and speech recognition, phonetics, prosody Digital signal processing, audio, acoustics, spatial hearing","title":"Home"},{"location":"3d-audio/","text":"University of Paris-Sud \u2014 Year 2 CS-Master (TC-2) Theoretical and practical material for the Fundamentals of Virtual and Augmented Reality class (TC2, Master 2 Research in Human-Computer Interaction). Download class part 1 Download class part 2 Download practical material Download practical instruction","title":"3D-Audio Class"},{"location":"3d-audio/#university-of-paris-sud-year-2-cs-master-tc-2","text":"Theoretical and practical material for the Fundamentals of Virtual and Augmented Reality class (TC2, Master 2 Research in Human-Computer Interaction). Download class part 1 Download class part 2 Download practical material Download practical instruction","title":"University of Paris-Sud \u2014 Year 2 CS-Master (TC-2)"},{"location":"audio-dsp/","text":"University of Paris-Sud \u2014 Year 5 CS-Engineers (ET-5) Theoretical and practical material files for the 2014-2015 Polytech\u2019 Paris-Sud Et5 Audio signal processing class. They are under the Ipython Notebook 2 format. Download class 1 material Download class 2 material Download class 3 material Download class 4 material","title":"Audio-DSP Class"},{"location":"audio-dsp/#university-of-paris-sud-year-5-cs-engineers-et-5","text":"Theoretical and practical material files for the 2014-2015 Polytech\u2019 Paris-Sud Et5 Audio signal processing class. They are under the Ipython Notebook 2 format. Download class 1 material Download class 2 material Download class 3 material Download class 4 material","title":"University of Paris-Sud \u2014 Year 5 CS-Engineers (ET-5)"},{"location":"contact/","text":"mevrard@ina.fr","title":"Contact"},{"location":"contact/#amp109amp101amp118amp114amp97amp114amp100amp64amp105amp110amp97amp46amp102amp114","text":"","title":"\u0002amp\u0003#109;\u0002amp\u0003#101;\u0002amp\u0003#118;\u0002amp\u0003#114;\u0002amp\u0003#97;\u0002amp\u0003#114;\u0002amp\u0003#100;\u0002amp\u0003#64;\u0002amp\u0003#105;\u0002amp\u0003#110;\u0002amp\u0003#97;\u0002amp\u0003#46;\u0002amp\u0003#102;\u0002amp\u0003#114;"},{"location":"cpp/","text":"University of Paris-Sud \u2014 Year 4 CS-Engineers (ET-4) Practicals and project presentation PDF files for the 2014-2015 Polytech\u2019 Paris-Sud Et4 info class. Download practical 1 Download practical 2 Download practical 3 Download practical 4 Download project Eclipse-EGit Tutorial","title":"C++ Class"},{"location":"cpp/#university-of-paris-sud-year-4-cs-engineers-et-4","text":"Practicals and project presentation PDF files for the 2014-2015 Polytech\u2019 Paris-Sud Et4 info class. Download practical 1 Download practical 2 Download practical 3 Download practical 4 Download project Eclipse-EGit Tutorial","title":"University of Paris-Sud \u2014 Year 4 CS-Engineers (ET-4)"},{"location":"cv/","text":"Doctor of Science in Computer Science Status (since 2018): Research and development engineer at the National Audiovisual Institute (INA), in France. Domains: Deep learning, Speech processing, NLP, digital humanities. Contact: mevrard@ina.fr Education 2012 -- 2015: Ph.D. in Computer Science at the LIMSI-CNRS -- University of Paris-Sud, in France. 2010: Preliminary complement to postgraduate education at the University of Li\u00e8ge, in Belgium. 2007: Audio engineering diploma at the SAE Institute Byron Bay, in Australia. 2002 -- 2004: Complementary Master in Business Administration at the Universit\u00e9 Libre de Bruxelles, in Belgium. 1997 -- 2002: Master of Science in Industrial engineering at the Gramme Institute (HELMo), in Belgium. Languages French: First language. English: Full professional proficiency. Dutch, Japanese: Basic. Other activities and Hobbies Music: Guitar, bass, and keyboard player in various rock and jazz bands since 1990. Audio engineering: Soundtrack production of 2 animated short-films (with camera-etc, Li\u00e8ge, Belgium). Recording, mixing, and production of Rock, Jazz, and Electronic music. Other: Photography, cinema, reading, martial arts.","title":"Short CV"},{"location":"cv/#education","text":"2012 -- 2015: Ph.D. in Computer Science at the LIMSI-CNRS -- University of Paris-Sud, in France. 2010: Preliminary complement to postgraduate education at the University of Li\u00e8ge, in Belgium. 2007: Audio engineering diploma at the SAE Institute Byron Bay, in Australia. 2002 -- 2004: Complementary Master in Business Administration at the Universit\u00e9 Libre de Bruxelles, in Belgium. 1997 -- 2002: Master of Science in Industrial engineering at the Gramme Institute (HELMo), in Belgium.","title":"Education"},{"location":"cv/#languages","text":"French: First language. English: Full professional proficiency. Dutch, Japanese: Basic.","title":"Languages"},{"location":"cv/#other-activities-and-hobbies","text":"Music: Guitar, bass, and keyboard player in various rock and jazz bands since 1990. Audio engineering: Soundtrack production of 2 animated short-films (with camera-etc, Li\u00e8ge, Belgium). Recording, mixing, and production of Rock, Jazz, and Electronic music. Other: Photography, cinema, reading, martial arts.","title":"Other activities and Hobbies"},{"location":"publi/","text":"M. Evrard, R. Uro, N. Herv\u00e9, and B. Mazoyer, \u201cFrench Tweet Corpus for Automatic StanceDetection,\u201d International Conference on Language Resources and Evaluation (LREC), 2020. R. Uro, M. Evrard, N. Herv\u00e9, and B. Mazoyer, \u201cThe Constitution of a French Tweet Corpus for Automatic Stance Detection,\u201d SLSP 2019, Ljubljana, Slovenia, 2019. D. Doukhan, E. Lechapt, M. Evrard, and J. Carrive, \u201cINA\u2019S MIREX 2018 music and speech detection system,\u201d in Music Information Retrieval Evaluation eXchange (MIREX), 2018. A. Rilliard, C. d\u2019Alessandro, and M. Evrard, \u201cParadigmatic variation of vowels in expressive speech: acoustic description and dimensional analysis,\u201d in Journal of the Acoustical Society of Americal (Jasa), 2018. M. Evrard, M. Miwa. and Y. Sasaki, \u201cSemantic graph embeddings and a neural language model for WSD,\u201d Second International Workshop on Symbolic-Neural Learning (SNL), 2018. M. Evrard, M. Miwa. and Y. Sasaki, \u201cTTI's Approaches to Symbolic-Neural Learning,\u201d First International Workshop on Symbolic-Neural Learning (SNL), 2017. M. Evrard, \u201cSynth\u00e8se de parole expressive \u00e0 partir du texte: Des phonostyles au contr\u00f4le gestuel pour la synth\u00e8se param\u00e9trique statistique,\u201d PhD thesis, Universit\u00e9 de Paris-Sud, 2015. M. Evrard, S. Delalez, C. d\u2019Alessandro, and A. Rilliard, \u201cComparison of chironomic stylization versus statistical modeling of prosody for expressive speech synthesis,\u201d in Sixteenth Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015. M. Evrard, C. d\u2019Alessandro, and A. Rilliard, \u201cEvaluation of the impact of corpus phonetic alignment on the hmm-based speech synthesis quality,\u201d in Statistical Language and Speech Processing, Springer, 2015. C.-T. Do, M. Evrard, A. Leman, C. d\u2019Alessandro, A. Rilliard, and J.-L. Crebouw, \u201cObjective evaluation of hmm-based speech synthesis system using Kullback-Leibler divergence,\u201d in Fifteenth Annual Conference of the International Speech Communication Association (INTERSPEECH), 2014. M. Evrard, C. R. Andr\u00e9, J. G. Verly, J.-J. Embrechts, and B. F. Katz, \u201cObject-based sound re-mix for the spatially coherent audio rendering of an existing stereoscopic-3D animation movie,\u201d in Audio Engineering Society Convention 131, Audio Engineering Society, New York, NY, United States, 2011. M. Evrard, C. Andr\u00e9, J. Verly, and J.-J. Embrechts, \u201cAdding wave-field-synthesis 3D audio to an existing stereoscopic-3D animation movie,\u201d in Third edition of the 3D Stereo MEDIA international summit, Li\u00e8ge, Belgium, 2011. M. Evrard, C. Andr\u00e9, J.-J. Embrechts, and J. Verly, \u201c3D audio acquisition and reproduction systems,\u201d in Journ\u00e9e ABAV (Association Belge des Acousticiens), Neder-over-Heembeek, Belgium, 2011. M. Evrard, A. Rilliard, and C. d\u2019Alessandro, \u201cReproduction de la personnalit\u00e9 vocale d\u2019un acteur,\u201d in Journ\u00e9es Jeunes Chercheurs en Audition, Acoustique musicale, et Signal audio (JJCAAS), Marseille, France, 2012. M. Evrard, A. Rilliard, and C. d\u2019Alessandro, \u201cCaract\u00e9risation et reproduction de la personnalit\u00e9 vocale d\u2019un acteur,\u201d in Journ\u00e9e des Doctorants du LIMSI-CNRS (JDD), Orsay, France, 2012.","title":"Publications"},{"location":"thesis/","text":"Expressive Text-to-Speech Synthesis From Phonostyles to Gestural Control for Parametric Statistic Synthesis (Written in French, title and abstract are here translated in English) The subject of this thesis was the study and conception of a platform for expressive speech synthesis. The LIPS\u00b3 Text-to-Speech system \u2014 developed in the context of this thesis \u2014 includes a linguistic module and a parametric statistical module (built upon HTS and STRAIGHT). The system was based on a new single-speaker corpus, designed, recorded and annotated. The first study analyzed the influence of the precision of the training corpus phonetic labeling on the synthesis quality. It showed that statistical parametric synthesis is robust to labeling and alignment errors. This addresses the issue of variation in phonetic realizations for expressive speech. The second study presents an acoustic-phonetic analysis of the corpus, characterizing the expressive space used by the speaker to instantiate the instructions that described the different expressive conditions. Voice source parameters and articulatory settings were analyzed according to their phonetic classes, which allowed for a fine phonostylistic characterization. The third study focused on intonation and rhythm. Calliphony 2.0 is a real-time chironomic interface that controls the f0 and rhythmic parameters of prosody, using drawing/writing hand gestures with a stylus and a graphics tablet. These hand-controlled modulations are used to enhance the TTS output, producing speech that is more realistic, without degradation as it is directly applied to the vocoder parameters. Intonation and rhythm stylization using this interface brings significant improvement to the prototypicality of expressivity, as well as to the general quality of synthetic speech. These studies show that parametric statistical synthesis, combined with a chironomic interface, offers an efficient solution for expressive speech synthesis, as well as a powerful tool for the study of prosody. Keywords: Expressive speech synthesis, gestural control, prosody, parametric statistical speech synthesis, adaptative training, HTS.","title":"Thesis"},{"location":"thesis/#expressive-text-to-speech-synthesis","text":"","title":"Expressive Text-to-Speech Synthesis"},{"location":"thesis/#from-phonostyles-to-gestural-control-for-parametric-statistic-synthesis","text":"(Written in French, title and abstract are here translated in English) The subject of this thesis was the study and conception of a platform for expressive speech synthesis. The LIPS\u00b3 Text-to-Speech system \u2014 developed in the context of this thesis \u2014 includes a linguistic module and a parametric statistical module (built upon HTS and STRAIGHT). The system was based on a new single-speaker corpus, designed, recorded and annotated. The first study analyzed the influence of the precision of the training corpus phonetic labeling on the synthesis quality. It showed that statistical parametric synthesis is robust to labeling and alignment errors. This addresses the issue of variation in phonetic realizations for expressive speech. The second study presents an acoustic-phonetic analysis of the corpus, characterizing the expressive space used by the speaker to instantiate the instructions that described the different expressive conditions. Voice source parameters and articulatory settings were analyzed according to their phonetic classes, which allowed for a fine phonostylistic characterization. The third study focused on intonation and rhythm. Calliphony 2.0 is a real-time chironomic interface that controls the f0 and rhythmic parameters of prosody, using drawing/writing hand gestures with a stylus and a graphics tablet. These hand-controlled modulations are used to enhance the TTS output, producing speech that is more realistic, without degradation as it is directly applied to the vocoder parameters. Intonation and rhythm stylization using this interface brings significant improvement to the prototypicality of expressivity, as well as to the general quality of synthetic speech. These studies show that parametric statistical synthesis, combined with a chironomic interface, offers an efficient solution for expressive speech synthesis, as well as a powerful tool for the study of prosody. Keywords: Expressive speech synthesis, gestural control, prosody, parametric statistical speech synthesis, adaptative training, HTS.","title":"From Phonostyles to Gestural Control for Parametric Statistic Synthesis"}]}